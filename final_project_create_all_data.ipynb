{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9a8574a",
   "metadata": {
    "id": "b9a8574a"
   },
   "source": [
    "# IR final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6c4555",
   "metadata": {
    "id": "ab6c4555"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776097b8",
   "metadata": {
    "id": "776097b8",
    "outputId": "f20cb6a1-1156-4e18-d78b-c834cd9e1c24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting google-cloud-storage==3.7.0\n",
      "  Downloading google_cloud_storage-3.7.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting google-auth<3.0.0,>=2.26.1 (from google-cloud-storage==3.7.0)\n",
      "  Downloading google_auth-2.45.0-py2.py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting google-api-core<3.0.0,>=2.27.0 (from google-cloud-storage==3.7.0)\n",
      "  Downloading google_api_core-2.28.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting google-cloud-core<3.0.0,>=2.4.2 (from google-cloud-storage==3.7.0)\n",
      "  Downloading google_cloud_core-2.5.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting google-resumable-media<3.0.0,>=2.7.2 (from google-cloud-storage==3.7.0)\n",
      "  Downloading google_resumable_media-2.8.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.22.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-cloud-storage==3.7.0) (2.25.1)\n",
      "Requirement already satisfied: google-crc32c<2.0.0,>=1.1.3 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-cloud-storage==3.7.0) (1.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage==3.7.0) (1.72.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage==3.7.0) (3.20.3)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage==3.7.0)\n",
      "  Downloading proto_plus-1.27.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage==3.7.0) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage==3.7.0) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/miniconda3/lib/python3.8/site-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage==3.7.0) (4.9)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage==3.7.0) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage==3.7.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage==3.7.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage==3.7.0) (2024.8.30)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.26.1->google-cloud-storage==3.7.0) (0.6.1)\n",
      "Downloading google_cloud_storage-3.7.0-py3-none-any.whl (303 kB)\n",
      "Downloading google_api_core-2.28.1-py3-none-any.whl (173 kB)\n",
      "Downloading google_auth-2.45.0-py2.py3-none-any.whl (233 kB)\n",
      "Downloading google_cloud_core-2.5.0-py3-none-any.whl (29 kB)\n",
      "Downloading google_resumable_media-2.8.0-py3-none-any.whl (81 kB)\n",
      "Downloading proto_plus-1.27.0-py3-none-any.whl (50 kB)\n",
      "Installing collected packages: proto-plus, google-resumable-media, google-auth, google-api-core, google-cloud-core, google-cloud-storage\n",
      "  Attempting uninstall: proto-plus\n",
      "    Found existing installation: proto-plus 1.11.0\n",
      "    Uninstalling proto-plus-1.11.0:\n",
      "      Successfully uninstalled proto-plus-1.11.0\n",
      "  Attempting uninstall: google-resumable-media\n",
      "    Found existing installation: google-resumable-media 1.3.3\n",
      "    Uninstalling google-resumable-media-1.3.3:\n",
      "      Successfully uninstalled google-resumable-media-1.3.3\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 1.35.0\n",
      "    Uninstalling google-auth-1.35.0:\n",
      "      Successfully uninstalled google-auth-1.35.0\n",
      "  Attempting uninstall: google-api-core\n",
      "    Found existing installation: google-api-core 1.34.1\n",
      "    Uninstalling google-api-core-1.34.1:\n",
      "      Successfully uninstalled google-api-core-1.34.1\n",
      "  Attempting uninstall: google-cloud-core\n",
      "    Found existing installation: google-cloud-core 1.7.3\n",
      "    Uninstalling google-cloud-core-1.7.3:\n",
      "      Successfully uninstalled google-cloud-core-1.7.3\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 1.35.1\n",
      "    Uninstalling google-cloud-storage-1.35.1:\n",
      "      Successfully uninstalled google-cloud-storage-1.35.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-cloud-bigquery 2.6.2 requires google-api-core[grpc]<2.0.0dev,>=1.23.0, but you have google-api-core 2.28.1 which is incompatible.\n",
      "google-cloud-bigquery 2.6.2 requires google-cloud-core<2.0dev,>=1.4.1, but you have google-cloud-core 2.5.0 which is incompatible.\n",
      "google-cloud-bigquery 2.6.2 requires google-resumable-media<2.0dev,>=0.6.0, but you have google-resumable-media 2.8.0 which is incompatible.\n",
      "google-cloud-bigquery-storage 2.1.0 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.28.1 which is incompatible.\n",
      "google-cloud-bigtable 1.6.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.28.1 which is incompatible.\n",
      "google-cloud-bigtable 1.6.1 requires google-cloud-core<2.0dev,>=1.4.1, but you have google-cloud-core 2.5.0 which is incompatible.\n",
      "google-cloud-container 2.3.1 requires google-api-core[grpc]<2.0.0dev,>=1.21.2, but you have google-api-core 2.28.1 which is incompatible.\n",
      "google-cloud-datacatalog 3.0.0 requires google-api-core[grpc]<2.0.0dev,>=1.22.0, but you have google-api-core 2.28.1 which is incompatible.\n",
      "google-cloud-dataproc 2.2.0 requires google-api-core[grpc]<2.0.0dev,>=1.22.0, but you have google-api-core 2.28.1 which is incompatible.\n",
      "google-cloud-language 2.0.0 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.28.1 which is incompatible.\n",
      "google-cloud-logging 2.1.1 requires google-api-core[grpc]<2.0.0dev,>=1.22.0, but you have google-api-core 2.28.1 which is incompatible.\n",
      "google-cloud-logging 2.1.1 requires google-cloud-core<2.0dev,>=1.4.1, but you have google-cloud-core 2.5.0 which is incompatible.\n",
      "google-cloud-monitoring 2.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.28.1 which is incompatible.\n",
      "google-cloud-pubsub 2.2.0 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.28.1 which is incompatible.\n",
      "google-cloud-redis 2.0.0 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.28.1 which is incompatible.\n",
      "google-cloud-spanner 2.1.1 requires proto-plus==1.11.0, but you have proto-plus 1.27.0 which is incompatible.\n",
      "google-cloud-speech 2.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.28.1 which is incompatible.\n",
      "google-cloud-texttospeech 2.2.0 requires google-api-core[grpc]<2.0.0dev,>=1.22.0, but you have google-api-core 2.28.1 which is incompatible.\n",
      "google-cloud-translate 3.0.2 requires google-api-core[grpc]<2.0.0dev,>=1.22.0, but you have google-api-core 2.28.1 which is incompatible.\n",
      "google-cloud-translate 3.0.2 requires google-cloud-core<2.0dev,>=1.1.0, but you have google-cloud-core 2.5.0 which is incompatible.\n",
      "google-cloud-vision 2.0.0 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.28.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-api-core-2.28.1 google-auth-2.45.0 google-cloud-core-2.5.0 google-cloud-storage-3.7.0 google-resumable-media-2.8.0 proto-plus-1.27.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.4.1%2Bcpu-cp38-cp38-linux_x86_64.whl (194.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.9/194.9 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.19.1%2Bcpu-cp38-cp38-linux_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m151.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.4.1%2Bcpu-cp38-cp38-linux_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/miniconda3/lib/python3.8/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/miniconda3/lib/python3.8/site-packages (from torch) (1.8)\n",
      "Requirement already satisfied: networkx in /opt/conda/miniconda3/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from torch) (3.0.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/miniconda3/lib/python3.8/site-packages (from torch) (0.9.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/miniconda3/lib/python3.8/site-packages (from torchvision) (1.19.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/miniconda3/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-2.4.1+cpu torchaudio-2.4.1+cpu torchvision-0.19.1+cpu\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/miniconda3/lib/python3.8/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from sentence-transformers) (2.4.1+cpu)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/miniconda3/lib/python3.8/site-packages (from sentence-transformers) (0.24.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/miniconda3/lib/python3.8/site-packages (from sentence-transformers) (1.6.3)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: Pillow in /opt/conda/miniconda3/lib/python3.8/site-packages (from sentence-transformers) (9.2.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/miniconda3/lib/python3.8/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/miniconda3/lib/python3.8/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/miniconda3/lib/python3.8/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.25.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/miniconda3/lib/python3.8/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: sympy in /opt/conda/miniconda3/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (1.8)\n",
      "Requirement already satisfied: networkx in /opt/conda/miniconda3/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/miniconda3/lib/python3.8/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.19.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/miniconda3/lib/python3.8/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2021.4.4)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.20.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/miniconda3/lib/python3.8/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/miniconda3/lib/python3.8/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/miniconda3/lib/python3.8/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading tokenizers-0.20.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, hf-xet, fsspec, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 0.9.0\n",
      "    Uninstalling fsspec-0.9.0:\n",
      "      Successfully uninstalled fsspec-0.9.0\n",
      "Successfully installed fsspec-2025.3.0 hf-xet-1.2.0 huggingface-hub-0.36.0 safetensors-0.5.3 sentence-transformers-3.2.1 tokenizers-0.20.3 transformers-4.46.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m3.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -q graphframes\n",
    "!pip install google-cloud-storage==3.7.0\n",
    "!pip install --no-cache-dir torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install sentence-transformers\n",
    "import google.cloud.storage\n",
    "print(google.cloud.storage.__version__)\n",
    "# !pip install --upgrade google-cloud-storage --ignore-installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b1579a",
   "metadata": {
    "id": "b0b1579a",
    "outputId": "d200e76b-81d7-4238-8dbf-682e1c09db79"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/miniconda3/lib/python3.8/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inverted_index_gcp.py\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import bz2\n",
    "import codecs\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import pyspark\n",
    "import sys\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *\n",
    "import pickle\n",
    "from google.cloud import storage\n",
    "import math\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from nltk.stem import PorterStemmer\n",
    "import pyspark.sql.functions as F \n",
    "from pyspark.sql.types import IntegerType, StructType, StructField\n",
    "from pyspark.sql.functions import sum as _sum, count as _count\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "%cd -q /home/dataproc\n",
    "!ls inverted_index_gcp.py\n",
    "# adding our python module to the cluster\n",
    "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
    "sys.path.insert(0,SparkFiles.getRootDirectory())\n",
    "from inverted_index_gcp import InvertedIndex\n",
    "\n",
    "import hashlib\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03edeb05",
   "metadata": {
    "id": "03edeb05"
   },
   "source": [
    "## Consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c513b0da",
   "metadata": {
    "id": "c513b0da"
   },
   "outputs": [],
   "source": [
    "# consts\n",
    "project_id = 'ir-final-project-12016'\n",
    "data_bucket_name = 'ir-final-project-bucket'\n",
    "output_folder = \"mappings\"\n",
    "storage_output_folder = \"prod\"\n",
    "page_view_map_filename = \"pageviews-202108-user.pkl\"\n",
    "pv_path = 'https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2'\n",
    "prod_file_name = f\"gs://{data_bucket_name}/multistream*\"\n",
    "final_index_filename = \"pageview_dict.pkl\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075d33e3",
   "metadata": {
    "id": "075d33e3"
   },
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d0cee4",
   "metadata": {
    "id": "40d0cee4",
    "outputId": "e51dee5a-1d20-4882-8c8a-db84bca72f27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://ir-final-project-bucket/multistream10_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream11_part2_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream11_preprocessed.parquet...      \n",
      "Copying gs://ir-final-project-bucket/multistream12_part2_preprocessed.parquet...\n",
      "/ [4 files][  1.2 GiB/  1.2 GiB]   76.2 MiB/s                                   \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying gs://ir-final-project-bucket/multistream12_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream13_part2_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream13_preprocessed.parquet...      \n",
      "Copying gs://ir-final-project-bucket/multistream14_part2_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream14_preprocessed.parquet...      \n",
      "Copying gs://ir-final-project-bucket/multistream15_part2_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream15_part3_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream15_preprocessed.parquet...      \n",
      "Copying gs://ir-final-project-bucket/multistream16_part2_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream16_part3_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream16_preprocessed.parquet...      \n",
      "Copying gs://ir-final-project-bucket/multistream17_part2_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream17_part3_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream17_preprocessed.parquet...      \n",
      "Copying gs://ir-final-project-bucket/multistream18_part2_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream18_part3_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream18_preprocessed.parquet...      \n",
      "Copying gs://ir-final-project-bucket/multistream19_part2_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream19_part3_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream19_preprocessed.parquet...      \n",
      "Copying gs://ir-final-project-bucket/multistream1_preprocessed.parquet...       \n",
      "Copying gs://ir-final-project-bucket/multistream20_part2_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream20_part3_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream20_preprocessed.parquet...      \n",
      "Copying gs://ir-final-project-bucket/multistream21_part2_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream21_part3_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream21_preprocessed.parquet...      \n",
      "Copying gs://ir-final-project-bucket/multistream22_part2_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream22_part3_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream22_part4_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream22_preprocessed.parquet...      \n",
      "Copying gs://ir-final-project-bucket/multistream23_part2_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream23_part3_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream23_part4_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream23_preprocessed.parquet...      \n",
      "Copying gs://ir-final-project-bucket/multistream24_part2_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream24_part3_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream24_part4_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream24_part5_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream24_preprocessed.parquet...      \n",
      "Copying gs://ir-final-project-bucket/multistream25_part2_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream25_part3_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream25_part4_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream25_preprocessed.parquet...      \n",
      "Copying gs://ir-final-project-bucket/multistream26_preprocessed.parquet...      \n",
      "Copying gs://ir-final-project-bucket/multistream27_part2_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream27_part3_preprocessed.parquet...\n",
      "Copying gs://ir-final-project-bucket/multistream27_preprocessed.parquet...      \n",
      "Copying gs://ir-final-project-bucket/multistream2_preprocessed.parquet...       \n",
      "Copying gs://ir-final-project-bucket/multistream3_preprocessed.parquet...       \n",
      "Copying gs://ir-final-project-bucket/multistream4_preprocessed.parquet...       \n",
      "Copying gs://ir-final-project-bucket/multistream5_preprocessed.parquet...       \n",
      "Copying gs://ir-final-project-bucket/multistream6_preprocessed.parquet...       \n",
      "Copying gs://ir-final-project-bucket/multistream7_preprocessed.parquet...       \n",
      "Copying gs://ir-final-project-bucket/multistream8_preprocessed.parquet...       \n",
      "Copying gs://ir-final-project-bucket/multistream9_preprocessed.parquet...       \n",
      "/ [60 files][ 14.3 GiB/ 14.3 GiB]  102.0 MiB/s                                  \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "\n",
      "Operation completed over 60 objects/14.3 GiB.                                    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Download page views\n",
    "!wget $pv_path\n",
    "# download data wikidumps\n",
    "!mkdir -p wikidumps\n",
    "!gsutil -u {project_id} cp gs://{data_bucket_name}/multistream*_preprocessed.parquet \"wikidumps/\"\n",
    "\n",
    "# load the data\n",
    "pages_links = spark.read.parquet(prod_file_name).select(\"anchor_text\", \"id\").rdd\n",
    "pages_title = spark.read.parquet(prod_file_name).select(\"title\", \"id\").rdd\n",
    "pages_body = spark.read.parquet(prod_file_name).select(\"text\", \"id\").rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14bb8a4",
   "metadata": {
    "id": "a14bb8a4"
   },
   "source": [
    "## Page View Task Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a52943",
   "metadata": {},
   "source": [
    "### Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c45dadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting execution...\n",
      "Processing file: pageviews-202108-user.bz2...\n",
      "Building dictionary in Python...\n",
      "Success! Saved to mappings/pageview_dict.pkl\n",
      "Uploading mappings/pageview_dict.pkl to gs://ir-final-project-bucket/prod/mappings/pageview_dict.pkl...\n",
      "Copying file://mappings/pageview_dict.pkl [Content-Type=application/octet-stream]...\n",
      "- [1 files][ 73.5 MiB/ 73.5 MiB]                                                \n",
      "Operation completed over 1 objects/73.5 MiB.                                     \n",
      "\n",
      "--- Test Results ---\n",
      "[335, 11706, 925]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "def read_page_view_and_create_map():\n",
    "    p = Path(pv_path)\n",
    "    pv_name = p.name  # pageviews-202108-user.bz2\n",
    "    pv_temp = f'{p.stem}-4dedup.txt'\n",
    "        \n",
    "    # the data wasnt downloaded\n",
    "    if not os.path.exists(pv_name):\n",
    "        print(f\"Error: File {pv_name} not found - download the data using wget.\")\n",
    "        return None\n",
    "    # Filter for English pages, and keep just two fields: article ID (3) and monthly\n",
    "    # total number of page views (5). Then, remove lines with article id or page\n",
    "    # view values that are not a sequence of digits.\n",
    "    command = f'bzcat {pv_name} | grep \"^en\\.wikipedia\" | cut -d\" \" -f3,5 | grep -P \"^\\d+\\s\\d+$\" > {pv_temp}'\n",
    "    exit_code = os.system(command)\n",
    "    \n",
    "    if exit_code != 0:\n",
    "        print(\"Error in shell command\")\n",
    "        return None\n",
    "\n",
    "    print(\"Building dictionary in Python...\")\n",
    "    # Create a Counter (dictionary) that sums up the pages views for the same\n",
    "    # article, resulting in a mapping from article id to total page views.\n",
    "    wid2pv = Counter()\n",
    "    \n",
    "    try:\n",
    "        with open(pv_temp, 'rt') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(' ')\n",
    "                wid2pv.update({int(parts[0]): int(parts[1])})\n",
    "                \n",
    "        pv_dict = dict(wid2pv)\n",
    "        \n",
    "        destination = f\"{output_folder}/{final_index_filename}\"\n",
    "        with open(destination, \"wb\") as f:\n",
    "            pickle.dump(pv_dict, f)\n",
    "                \n",
    "        if os.path.exists(pv_temp):\n",
    "            os.remove(pv_temp)\n",
    "            \n",
    "        return pv_dict\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during python processing: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Starting execution...\")\n",
    "dictionary_result = read_page_view_and_create_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab21af77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload ouput page view to storage\n",
    "index_src = f\"{output_folder}/{final_index_filename}\"\n",
    "index_dst = f'gs://ir-final-project-bucket/{storage_output_folder}/mappings/{final_index_filename}'\n",
    "print(f\"Uploading {index_src} to {index_dst}...\")\n",
    "!gsutil cp $index_src $index_dst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0abb2a",
   "metadata": {
    "id": "fa0abb2a"
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b99059",
   "metadata": {
    "id": "b4b99059"
   },
   "outputs": [],
   "source": [
    "# load\n",
    "with open(f\"{output_folder}/pageview_dict.pkl\", 'rb') as f:\n",
    "    wid2pv = pickle.loads(f.read())\n",
    "\n",
    "\n",
    "def get_pageview(wiki_ids: List[int]) -> List[int]:\n",
    "  return [wid2pv[wiki_id] for wiki_id in wiki_ids]\n",
    "\n",
    "#check func\n",
    "get_pageview([27960185, 1232201, 1836501])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbadd1bd",
   "metadata": {
    "id": "cbadd1bd"
   },
   "source": [
    "## Page Rank Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d95564f",
   "metadata": {
    "id": "2d95564f"
   },
   "source": [
    "### Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b744d9",
   "metadata": {
    "id": "76b744d9",
    "outputId": "9e33e3bf-e40c-4e03-ac96-a73774b7ae23"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def extract_edges_from_row(row):\n",
    "  #row: (source_id, list_of_anchors)\n",
    "  #list_of_anchors: [(destination_id, link_text),...]\n",
    "    source_id = row[1]\n",
    "    list_of_anchors = row[0]\n",
    "    edges = []\n",
    "    # iterate through the anchors list to extract destination IDs\n",
    "    if list_of_anchors is not None:\n",
    "        for anchor in list_of_anchors:\n",
    "            destination_id = anchor[0]\n",
    "            edges.append((source_id, destination_id))\n",
    "\n",
    "    return edges\n",
    "\n",
    "def get_first_element(edge):\n",
    "    return edge[0]\n",
    "\n",
    "def get_second_element(edge):\n",
    "    return edge[1]\n",
    "\n",
    "def generate_graph(pages):\n",
    "    ''' Compute the directed graph generated by wiki links.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    pages: RDD\n",
    "      An RDD where each row consists of one wikipedia articles with 'id' and\n",
    "      'anchor_text'.\n",
    "    Returns:\n",
    "    --------\n",
    "    edges: RDD\n",
    "      An RDD where each row represents an edge in the directed graph created by\n",
    "      the wikipedia links. The first entry should the source page id and the\n",
    "      second entry is the destination page id. No duplicates should be present.\n",
    "    vertices: RDD\n",
    "      An RDD where each row represents a vetrix (node) in the directed graph\n",
    "      created by the wikipedia links. No duplicates should be present.\n",
    "    '''\n",
    "    # compute edges: transform each page into a list of individual edges -> (source_id, destination_id) pairs\n",
    "    edges_with_duplicates = pages.flatMap(extract_edges_from_row)\n",
    "\n",
    "    # remove duplicate edges\n",
    "    edges = edges_with_duplicates.distinct()\n",
    "\n",
    "    # compute Vertices: extract all source IDs from the edges\n",
    "    sources = edges.map(get_first_element)\n",
    "\n",
    "    # extract all destination IDs from the edges\n",
    "    destinations = edges.map(get_second_element)\n",
    "\n",
    "    # union both lists and removes duplicates\n",
    "    vertices = sources.union(destinations).distinct().map(lambda x: (x,))\n",
    "\n",
    "    return edges, vertices\n",
    "\n",
    "\n",
    "def create_page_rank(pages_df):\n",
    "    # construct the graph\n",
    "    edges, vertices = generate_graph(pages_df)\n",
    "    # compute PageRank\n",
    "    edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n",
    "    verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n",
    "    g = GraphFrame(verticesDF, edgesDF)\n",
    "    pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n",
    "    pr = pr_results.vertices.select(\"id\", \"pagerank\")\n",
    "    pr = pr.sort(col('pagerank').desc())\n",
    "    pr_pd = pr.toPandas()\n",
    "#     pr.write.mode(\"overwrite\").parquet(pr_path)\n",
    "    pr_dict = dict(zip(pr_pd['id'].astype(int), pr_pd['pagerank']))\n",
    "    with open(f\"{output_folder}/pagerank_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(pr_dict, f)\n",
    "    storage.Client().bucket(data_bucket_name).blob(f\"{storage_output_folder}/mappings/pagerank_dict.pkl\").upload_from_filename(\n",
    "        f\"{output_folder}/pagerank_dict.pkl\"\n",
    "    )\n",
    "\n",
    "create_page_rank(pages_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3492daf",
   "metadata": {
    "id": "d3492daf"
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dfd0ae",
   "metadata": {
    "id": "55dfd0ae",
    "outputId": "192e8dc7-6c90-4e57-e37d-e2eebe5e75de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1959.6382627292862, 2782.03964641377]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load\n",
    "storage.Client().bucket(data_bucket_name).blob(f\"{storage_output_folder}/mappings/pagerank_dict.pkl\").download_to_filename(\"pagerank_dict.pkl\")\n",
    "with open(\"pagerank_dict.pkl\", \"rb\") as f:\n",
    "    PAGERANK_DICT = pickle.load(f)\n",
    "def get_pagerank(wiki_ids: List[int]) -> List[float]:\n",
    "    res = [PAGERANK_DICT.get(int(wid), 0.0) for wid in wiki_ids]\n",
    "    return res\n",
    "\n",
    "#check\n",
    "get_pagerank([21241, 21148, 9316])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3035cf11",
   "metadata": {
    "id": "3035cf11"
   },
   "source": [
    "## Inverted Index Creation - Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29910d9",
   "metadata": {
    "id": "d29910d9"
   },
   "outputs": [],
   "source": [
    "client = storage.Client()\n",
    "\n",
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
    "                    \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "\n",
    "NUM_BUCKETS = 124\n",
    "def token2bucket_id(token):\n",
    "    return int(_hash(token),16) % NUM_BUCKETS\n",
    "\n",
    "if not os.path.exists(data_bucket_name):\n",
    "    os.makedirs(data_bucket_name)\n",
    "\n",
    "# toknization for the basic inverted indecies\n",
    "def simple_tokenize(full_text: str) -> List[str]:\n",
    "    return  [\n",
    "        tok.group().lower()\n",
    "        for tok in RE_WORD.finditer(full_text)\n",
    "        if tok.group().lower() not in all_stopwords\n",
    "    ]\n",
    "\n",
    "#warpper function for the toknization function\n",
    "def tokenize_text(text_element, tokenize_func):\n",
    "    if isinstance(text_element, list):\n",
    "        if not text_element:\n",
    "            return []\n",
    "        texts = []\n",
    "        for r in text_element:\n",
    "            if hasattr(r, \"text\") and isinstance(r.text, str):\n",
    "                texts.append(r.text)\n",
    "        if not texts:\n",
    "            return []\n",
    "        full_text = \" \".join(texts)\n",
    "    else:\n",
    "        full_text = text_element\n",
    "\n",
    "    return tokenize_func(full_text)\n",
    "\n",
    "def word_count(text_element, doc_id, tokenize_func):\n",
    "    tf_counter = Counter(tokenize_text(text_element, tokenize_func))\n",
    "    return [(token, (doc_id, tf)) for token, tf in tf_counter.items()]\n",
    "\n",
    "def reduce_word_counts(text_element):\n",
    "    return sorted(text_element, key=lambda x: x[0])\n",
    "\n",
    "def calculate_df(postings):\n",
    "    return postings.mapValues(lambda pl: len(pl))\n",
    "\n",
    "def map_to_bucket_key(item):\n",
    "    \"\"\" Transforms (token, posting_list) into (bucket_id, (token, posting_list)). \"\"\"\n",
    "\n",
    "    # item[0] is the token\n",
    "    bucket_id = token2bucket_id(item[0])\n",
    "\n",
    "    # the value is the original item itself\n",
    "    return (bucket_id, item)\n",
    "\n",
    "def write_and_return_locations(bucket_id_and_postings, index_name: str):\n",
    "    \"\"\" Writes one bucket's postings to disk and returns the location map. \"\"\"\n",
    "    os.makedirs(data_bucket_name, exist_ok=True)\n",
    "    # unpack the input: (bucket_id, iterable_of_postings)\n",
    "    bucket_id, postings_iterable = bucket_id_and_postings\n",
    "\n",
    "    # [(w, posting_list), (w, posting_list)....]\n",
    "    bucket_postings_list = list(postings_iterable)\n",
    "    if not bucket_postings_list:\n",
    "        return []\n",
    "\n",
    "    posting_locations_dict = InvertedIndex.write_a_posting_list((bucket_id, bucket_postings_list), f\"{storage_output_folder}/{index_name}_gcp\", data_bucket_name)\n",
    "    return posting_locations_dict\n",
    "\n",
    "def partition_postings_and_write(postings, index_name: str):\n",
    "    ''' A function that partitions the posting lists into buckets, writes out\n",
    "    all posting lists in a bucket to disk, and returns the posting locations for\n",
    "    each bucket. Partitioning should be done through the use of `token2bucket`\n",
    "    above. Writing to disk should use the function  `write_a_posting_list`, a\n",
    "    static method implemented in inverted_index_colab.py under the InvertedIndex\n",
    "    class.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    postings: RDD\n",
    "      An RDD where each item is a (w, posting_list) pair.\n",
    "    Returns:\n",
    "    --------\n",
    "    RDD\n",
    "      An RDD where each item is a posting locations dictionary for a bucket. The\n",
    "      posting locations maintain a list for each word of file locations and\n",
    "      offsets its posting list was written to. See `write_a_posting_list` for\n",
    "      more details.\n",
    "    '''\n",
    "    # add the bucket_id as the key -> (bucket_id, (w, posting_list))\n",
    "    mapped_rdd = postings.map(map_to_bucket_key)\n",
    "\n",
    "    # collect all items belonging to the same bucket_id -> (bucket_id, iterator of (w, posting_list))\n",
    "    grouped_rdd = mapped_rdd.groupByKey()\n",
    "\n",
    "    # apply the disk writing function to each grouped bucket\n",
    "    # return information and the location of the data in the disk\n",
    "    return grouped_rdd.map(lambda x: write_and_return_locations(x, index_name))\n",
    "\n",
    "def create_inverted_index(pages_data: pyspark.rdd.RDD, index_name: str, min_elements_in_doc: int, tokenize_func = simple_tokenize):\n",
    "    word_counts = pages_data.flatMap(lambda x: word_count(x[0], x[1], tokenize_func))\n",
    "    postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
    "    # filtering postings and calculate df\n",
    "    postings_filtered = postings.filter(lambda x: len(x[1])>min_elements_in_doc)\n",
    "    \n",
    "    postings_filtered.persist()\n",
    "    print(f\"Writing index {index_name} to disk...\")\n",
    "    # partition posting lists and write out\n",
    "    _ = partition_postings_and_write(postings_filtered, index_name).collect()\n",
    "    \n",
    "    print(\"Calculating and collecting DF...\")\n",
    "    w2df = calculate_df(postings_filtered)\n",
    "    w2df_dict = w2df.collectAsMap()\n",
    "\n",
    "    inverted = InvertedIndex()\n",
    "\n",
    "    super_posting_locs = defaultdict(list)\n",
    "    for blob in client.list_blobs(data_bucket_name, prefix=f\"{storage_output_folder}/{index_name}_gcp\"):\n",
    "        if not blob.name.endswith(\"pickle\"):\n",
    "            continue\n",
    "        with blob.open(\"rb\") as f:\n",
    "            posting_locs = pickle.load(f)\n",
    "        for k, v in posting_locs.items():\n",
    "            super_posting_locs[k].extend(v)\n",
    "\n",
    "    inverted.posting_locs = super_posting_locs\n",
    "    inverted.df = w2df_dict\n",
    "\n",
    "    # write the global stats out\n",
    "    inverted.write_index('.', index_name)\n",
    "    index_src = f\"{index_name}.pkl\"\n",
    "    index_dst = f'gs://{data_bucket_name}/{storage_output_folder}/inverted_indecies/{index_src}'\n",
    "    !gsutil cp $index_src $index_dst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55de1de",
   "metadata": {
    "id": "a55de1de"
   },
   "source": [
    "### all basic index creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d4210",
   "metadata": {
    "id": "559d4210"
   },
   "outputs": [],
   "source": [
    "create_inverted_index(pages_links, \"ancher_inverted_index\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cff3b99",
   "metadata": {
    "id": "3cff3b99"
   },
   "outputs": [],
   "source": [
    "create_inverted_index(pages_title, \"title_inverted_index\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9316ce20",
   "metadata": {
    "id": "9316ce20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing index body_inverted_index to disk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve RDD 37\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\n",
      "\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\n",
      "\tat java.net.ServerSocket.accept(ServerSocket.java:528)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:64)\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_35 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_123 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_1 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_96 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_12 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_88 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_4 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_121 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_46 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_43 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_38 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_65 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_83 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_17 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_24 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_8 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_61 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_115 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_72 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_21 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_69 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_103 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_50 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_30 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_77 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_92 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_107 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_113 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_99 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_54 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_28 !\n",
      "25/12/25 12:42:33 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1766644774302_0006_01_000005 on host: cluster-0016-w-1.us-central1-a.c.ir-final-project-12016.internal. Exit status: 143. Diagnostics: [2025-12-25 12:42:33.675]Container killed on request. Exit code is 143\n",
      "[2025-12-25 12:42:33.675]Container exited with a non-zero exit code 143. \n",
      "[2025-12-25 12:42:33.676]Killed by external signal\n",
      ".\n",
      "25/12/25 12:42:33 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 4 for reason Container from a bad node: container_1766644774302_0006_01_000005 on host: cluster-0016-w-1.us-central1-a.c.ir-final-project-12016.internal. Exit status: 143. Diagnostics: [2025-12-25 12:42:33.675]Container killed on request. Exit code is 143\n",
      "[2025-12-25 12:42:33.675]Container exited with a non-zero exit code 143. \n",
      "[2025-12-25 12:42:33.676]Killed by external signal\n",
      ".\n",
      "25/12/25 12:42:33 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 4 on cluster-0016-w-1.us-central1-a.c.ir-final-project-12016.internal: Container from a bad node: container_1766644774302_0006_01_000005 on host: cluster-0016-w-1.us-central1-a.c.ir-final-project-12016.internal. Exit status: 143. Diagnostics: [2025-12-25 12:42:33.675]Container killed on request. Exit code is 143\n",
      "[2025-12-25 12:42:33.675]Container exited with a non-zero exit code 143. \n",
      "[2025-12-25 12:42:33.676]Killed by external signal\n",
      ".\n",
      "25/12/25 12:42:33 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 40.0 in stage 10.0 (TID 719) (cluster-0016-w-1.us-central1-a.c.ir-final-project-12016.internal executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container from a bad node: container_1766644774302_0006_01_000005 on host: cluster-0016-w-1.us-central1-a.c.ir-final-project-12016.internal. Exit status: 143. Diagnostics: [2025-12-25 12:42:33.675]Container killed on request. Exit code is 143\n",
      "[2025-12-25 12:42:33.675]Container exited with a non-zero exit code 143. \n",
      "[2025-12-25 12:42:33.676]Killed by external signal\n",
      ".\n",
      "25/12/25 12:42:33 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 43.0 in stage 10.0 (TID 722) (cluster-0016-w-1.us-central1-a.c.ir-final-project-12016.internal executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container from a bad node: container_1766644774302_0006_01_000005 on host: cluster-0016-w-1.us-central1-a.c.ir-final-project-12016.internal. Exit status: 143. Diagnostics: [2025-12-25 12:42:33.675]Container killed on request. Exit code is 143\n",
      "[2025-12-25 12:42:33.675]Container exited with a non-zero exit code 143. \n",
      "[2025-12-25 12:42:33.676]Killed by external signal\n",
      ".\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_73 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_20 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_51 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_6 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_62 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_34 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_119 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_79 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_15 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_32 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_114 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_11 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_40 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_49 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_81 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_122 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_2 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_42 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_27 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_97 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_95 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_106 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_56 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_70 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_104 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_63 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_57 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_87 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_22 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_109 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_42_86 !\n",
      "25/12/25 12:43:14 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1766644774302_0006_01_000002 on host: cluster-0016-w-0.us-central1-a.c.ir-final-project-12016.internal. Exit status: 143. Diagnostics: [2025-12-25 12:43:14.629]Container killed on request. Exit code is 143\n",
      "[2025-12-25 12:43:14.630]Container exited with a non-zero exit code 143. \n",
      "[2025-12-25 12:43:14.634]Killed by external signal\n",
      ".\n",
      "25/12/25 12:43:14 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 2 on cluster-0016-w-0.us-central1-a.c.ir-final-project-12016.internal: Container from a bad node: container_1766644774302_0006_01_000002 on host: cluster-0016-w-0.us-central1-a.c.ir-final-project-12016.internal. Exit status: 143. Diagnostics: [2025-12-25 12:43:14.629]Container killed on request. Exit code is 143\n",
      "[2025-12-25 12:43:14.630]Container exited with a non-zero exit code 143. \n",
      "[2025-12-25 12:43:14.634]Killed by external signal\n",
      ".\n",
      "25/12/25 12:43:14 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 56.0 in stage 10.0 (TID 737) (cluster-0016-w-0.us-central1-a.c.ir-final-project-12016.internal executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container from a bad node: container_1766644774302_0006_01_000002 on host: cluster-0016-w-0.us-central1-a.c.ir-final-project-12016.internal. Exit status: 143. Diagnostics: [2025-12-25 12:43:14.629]Container killed on request. Exit code is 143\n",
      "[2025-12-25 12:43:14.630]Container exited with a non-zero exit code 143. \n",
      "[2025-12-25 12:43:14.634]Killed by external signal\n",
      ".\n",
      "25/12/25 12:43:14 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 55.0 in stage 10.0 (TID 736) (cluster-0016-w-0.us-central1-a.c.ir-final-project-12016.internal executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container from a bad node: container_1766644774302_0006_01_000002 on host: cluster-0016-w-0.us-central1-a.c.ir-final-project-12016.internal. Exit status: 143. Diagnostics: [2025-12-25 12:43:14.629]Container killed on request. Exit code is 143\n",
      "[2025-12-25 12:43:14.630]Container exited with a non-zero exit code 143. \n",
      "[2025-12-25 12:43:14.634]Killed by external signal\n",
      ".\n",
      "25/12/25 12:43:14 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 2 for reason Container from a bad node: container_1766644774302_0006_01_000002 on host: cluster-0016-w-0.us-central1-a.c.ir-final-project-12016.internal. Exit status: 143. Diagnostics: [2025-12-25 12:43:14.629]Container killed on request. Exit code is 143\n",
      "[2025-12-25 12:43:14.630]Container exited with a non-zero exit code 143. \n",
      "[2025-12-25 12:43:14.634]Killed by external signal\n",
      ".\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating and collecting DF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://body_inverted_index.pkl [Content-Type=application/octet-stream]...\n",
      "/ [1 files][ 18.5 MiB/ 18.5 MiB]                                                \n",
      "Operation completed over 1 objects/18.5 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "create_inverted_index(pages_body, \"body_inverted_index\", 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8300eba0",
   "metadata": {
    "id": "8300eba0"
   },
   "source": [
    "### Basic Search predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43543522",
   "metadata": {
    "id": "43543522"
   },
   "outputs": [],
   "source": [
    "def load_resources():\n",
    "    client = storage.Client()\n",
    "    files = [\"titles_dict.pkl\", \"anchor_stats.pkl\", \"title_stats.pkl\", \"body_stats.pkl\"]\n",
    "    for file_name in files:\n",
    "        client.bucket(data_bucket_name).blob(f\"{storage_output_folder}/stats/{file_name}.pkl\").download_to_filename(file_name)\n",
    "    with open('body_stats.pkl', 'rb') as f:\n",
    "        body_data = pickle.load(f)\n",
    "    with open('title_stats.pkl', 'rb') as f:\n",
    "        title_data = pickle.load(f)\n",
    "    with open('anchor_stats.pkl', 'rb') as f:\n",
    "        anchor_data = pickle.load(f)\n",
    "    with open('titles_dict.pkl', 'rb') as f:\n",
    "        titles_dict = pickle.load(f)\n",
    "    client.bucket(data_bucket_name).blob(f\"{storage_output_folder}/mappings/pagerank_dict.pkl\").download_to_filename(\"pagerank_dict.pkl\")\n",
    "    with open(\"pagerank_dict.pkl\", \"rb\") as f:\n",
    "        pagerank_dict = pickle.load(f)\n",
    "    client.bucket(data_bucket_name).blob(f\"{storage_output_folder}/mappings/pageview_dict.pkl\").download_to_filename(\"pageview_dict.pkl\")\n",
    "    with open(\"pageview_dict.pkl\", \"rb\") as f:\n",
    "        wid2pv = pickle.load(f)\n",
    "\n",
    "    return body_data, title_data, anchor_data, titles_dict, pagerank_dict, wid2pv\n",
    "\n",
    "\n",
    "BODY_STATS, TITLE_STATS, ANCHOR_STATS, TITLES_DICT, PAGERANK_DICT, WID2PV = load_resources()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9573818",
   "metadata": {
    "id": "d9573818",
    "outputId": "7567c39c-3f06-4635-d744-e5beacd1a32b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://final-project-bucket-ir/inverted_indecies_test/body_inverted_index.pkl...\n",
      "/ [1 files][  7.6 MiB/  7.6 MiB]                                                \n",
      "Operation completed over 1 objects/7.6 MiB.                                      \n",
      "Copying gs://final-project-bucket-ir/inverted_indecies_test/title_inverted_index.pkl...\n",
      "/ [1 files][433.2 KiB/433.2 KiB]                                                \n",
      "Operation completed over 1 objects/433.2 KiB.                                    \n",
      "Copying gs://final-project-bucket-ir/inverted_indecies_test/ancher_inverted_index.pkl...\n",
      "/ [1 files][  3.1 MiB/  3.1 MiB]                                                \n",
      "Operation completed over 1 objects/3.1 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "# read the inverted index from storage\n",
    "def read_inverted_index(index_name: str):\n",
    "    # read the inverted index from storage\n",
    "    index_src = f'gs://{data_bucket_name}/{storage_output_folder}/inverted_indecies/{index_name}.pkl'\n",
    "    index_dst = f\"{index_name}.pkl\"\n",
    "    !gsutil cp $index_src $index_dst\n",
    "    with open(index_dst, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "body_inverted_index = read_inverted_index(\"body_inverted_index\")\n",
    "title_inverted_index = read_inverted_index(\"title_inverted_index\")\n",
    "inverted_anchor_index = read_inverted_index(\"ancher_inverted_index\")\n",
    "\n",
    "def search_helper(query: str, inverted_index):\n",
    "    query_tokens = {\n",
    "        tok.group().lower()\n",
    "        for tok in RE_WORD.finditer(query)\n",
    "        if tok.group().lower() not in all_stopwords\n",
    "    }\n",
    "    if not query_tokens:\n",
    "        return []\n",
    "\n",
    "    # Map doc_id -> set of query tokens that appear in its anchor text\n",
    "    doc_seen_per_token = defaultdict(set)\n",
    "\n",
    "    # Only process posting lists for terms in the query\n",
    "    for term in query_tokens:\n",
    "        posting_list = inverted_index.read_a_posting_list('.', term, data_bucket_name)\n",
    "        for doc_id, tf in posting_list:\n",
    "            doc_seen_per_token[doc_id].add(term)\n",
    "\n",
    "    # Count distinct query words per document\n",
    "    doc2count = [(doc_id, len(token_set)) for doc_id, token_set in doc_seen_per_token.items()]\n",
    "\n",
    "    # Sort by number of distinct query words descending\n",
    "    doc2count.sort(key=lambda x: -x[1])\n",
    "    if len(doc2count) == 0:\n",
    "        return []\n",
    "    res = []\n",
    "    for doc_id, count in doc2count:\n",
    "        title = TITLES_DICT.get(str(doc_id), \"Unknown\")\n",
    "        res.append((str(doc_id), title))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f915998d",
   "metadata": {
    "id": "f915998d"
   },
   "outputs": [],
   "source": [
    "def search_title(query: str) -> List[Tuple[str, str]]:\n",
    "    return search_helper(query, title_inverted_index)\n",
    "\n",
    "def search_anchor(query: str) -> List[Tuple[str, str]]:\n",
    "    return search_helper(query, inverted_anchor_index)\n",
    "\n",
    "def search_body(query: str) -> List[Tuple[str, str]]:\n",
    "    return search_helper(query, body_inverted_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc0f275",
   "metadata": {
    "id": "4dc0f275"
   },
   "source": [
    "## Multi Search Data Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93eb5de",
   "metadata": {
    "id": "b93eb5de"
   },
   "source": [
    "### Inverted Index with better tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebac972c",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "7ed3b099e0c640558505e02498f2a025",
      "465c650a720642c19e1c13dbfdb78385",
      "9916afb858a7464c8cc0e303a1d4ea42",
      "054814e6405649a48b562f7c65cd9a33",
      "6e58b762a2d44ee8a4a4d8fc359c76c4",
      "8bb77013c16b409eb7138d41da7aee01",
      "42b5d324e9694110aaf88a03a62d189a",
      "0996ed8662074759b81a1deec00772a3",
      "ea890b16bf214492809063d546db62b3",
      "14fd84334e9342e9aaf29bb6fa101c36",
      "70a28b090405427ea310a5b36f08514a"
     ]
    },
    "id": "ebac972c",
    "outputId": "d692584b-f938-4d11-91cd-c52ec3eb5be1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f140084934a649abbb7442789828876e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aad69d303014890a028520a9281f064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44cf45898ae44269868a43d737e3edc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc66c0012d964c1a9cc607ef85aa58e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2545dde2d4f842c88d3951e892f42316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1714f96f813c416da33a3dd0efcd8d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4eae29253b4b2cb1332bf54145c38e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c035f1d2975c47d5a749998ce8fb017d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d45a4e8dad349e4a78e52b72babcc99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "137ac3103d6f4b408b7282be9751010f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac73eba443a4598bd2dac48dd5aa312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "MAX_BIGRAMS = 30     \n",
    "MIN_TOKEN_LEN = 3\n",
    "\n",
    "def tokenize_anchor(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [\n",
    "        # stemming\n",
    "        stemmer.stem(tok.group().lower())\n",
    "        for tok in RE_WORD.finditer(text)\n",
    "        if tok.group().lower() not in all_stopwords\n",
    "        and len(tok.group()) >= MIN_TOKEN_LEN\n",
    "    ]\n",
    "\n",
    "def tokenize_smart(text):\n",
    "    # stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    # basic tokenize \n",
    "    tokens = [tok.group().lower()for tok in RE_WORD.finditer(text)]\n",
    "    tokens = [stemmer.stem(tok)for tok in tokens if tok not in all_stopwords and len(tok) >= MIN_TOKEN_LEN]\n",
    "    # create bigrams\n",
    "    bigrams = []\n",
    "    for i in range(len(tokens) - 1):\n",
    "        if len(bigrams) >= MAX_BIGRAMS:\n",
    "            break\n",
    "        t1, t2 = tokens[i], tokens[i + 1]\n",
    "        if t1.isalpha() and t2.isalpha():\n",
    "            bigrams.append(f\"{t1}_{t2}\")\n",
    "    return tokens + bigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621c180e",
   "metadata": {
    "id": "621c180e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve RDD 34\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\n",
      "\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\n",
      "\tat java.net.ServerSocket.accept(ServerSocket.java:528)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:64)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://ancher_inverted_index_smart.pkl [Content-Type=application/octet-stream]...\n",
      "/ [1 files][ 21.4 MiB/ 21.4 MiB]                                                \n",
      "Operation completed over 1 objects/21.4 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "create_inverted_index(pages_links, \"ancher_inverted_index_smart\", 10, tokenize_func=tokenize_anchor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566416e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://title_inverted_index_smart.pkl [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "/ [1 files][279.9 MiB/279.9 MiB]                                                \n",
      "Operation completed over 1 objects/279.9 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "create_inverted_index(pages_title, \"title_inverted_index_smart\", 0, tokenize_func=tokenize_smart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7cbac3",
   "metadata": {
    "id": "0b7cbac3"
   },
   "source": [
    "### avg text len map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1df402c",
   "metadata": {
    "id": "d1df402c"
   },
   "outputs": [],
   "source": [
    "def count_tokens_fast(text_element):\n",
    "    if text_element is None:\n",
    "        return 0   \n",
    "    # handeling anchor\n",
    "    if isinstance(text_element, list):\n",
    "        text_parts = []\n",
    "        for item in text_element:\n",
    "            if isinstance(item, str):\n",
    "                text_parts.append(item)\n",
    "            elif hasattr(item, \"text\") and item.text:\n",
    "                text_parts.append(item.text)\n",
    "            elif isinstance(item, (list, tuple)) and len(item) > 1: \n",
    "                 text_parts.append(str(item[1]))\n",
    "        \n",
    "        text_element = \" \".join(text_parts)\n",
    "    \n",
    "    # another final check\n",
    "    if not isinstance(text_element, str):\n",
    "        return 0\n",
    "\n",
    "    #extracting tokens and filtering stop words    \n",
    "    tokens = [tok.group().lower() for tok in RE_WORD.finditer(text_element)]\n",
    "    tokens = [tok for tok in tokens if tok not in all_stopwords and len(tok) >= MIN_TOKEN_LEN]\n",
    "\n",
    "    bigrams_count = 0\n",
    "    token_count = len(tokens)\n",
    "    # creating bigrams\n",
    "    for i in range(token_count - 1):\n",
    "        if bigrams_count >= MAX_BIGRAMS:\n",
    "            break\n",
    "            \n",
    "        if tokens[i].isalpha() and tokens[i+1].isalpha(): # bigram should be an actual word and not a number\n",
    "            bigrams_count += 1\n",
    "            \n",
    "    return token_count + bigrams_count\n",
    "\n",
    "# bm25 needs avg doc len so we calcluate it for body, title and anchor\n",
    "def calculate_field_stats(rdd, field_name):\n",
    "    print(f\"Starting stats calculation for {field_name}...\")\n",
    "    parquet_path = f\"dl_{field_name}\"\n",
    "    \n",
    "    if field_name == 'body': # too much data for body so we go with the statstics\n",
    "        print(\"Sampling 10% of the body data to speed up calculation...\")\n",
    "        rdd = rdd.sample(False, 0.1, seed=42)\n",
    "    \n",
    "    dl_rdd = rdd.map(lambda x: (str(x[1]), count_tokens_fast(x[0])))\n",
    "    dl_rdd.toDF([\"doc_id\", \"dl\"]).write.mode(\"overwrite\").parquet(parquet_path)\n",
    "    \n",
    "    df_stats = spark.read.parquet(parquet_path)\n",
    "    agg_results = df_stats.select(\n",
    "        _sum(\"dl\").alias(\"total_len\"), \n",
    "        _count(\"dl\").alias(\"doc_count\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    total_len = agg_results[\"total_len\"]\n",
    "    sample_doc_count = agg_results[\"doc_count\"]\n",
    "    \n",
    "    avg_len = total_len / sample_doc_count if sample_doc_count else 0\n",
    "    real_doc_count = sample_doc_count * 10 if field_name == 'body' else sample_doc_count\n",
    "    \n",
    "    stats = {\"avgdl\": avg_len, \"doc_count\": real_doc_count}\n",
    "    filename = f\"{field_name}_stats.pkl\"\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(stats, f)\n",
    "        \n",
    "    print(f\"Finished {field_name}: avgdl={avg_len}, est_count={real_doc_count}\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfabfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting stats calculation for body...\n",
      "Sampling 10% of the body data to speed up calculation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:===========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished body: avgdl=346.72521575500065, est_count=6345160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'body_stats.pkl'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_field_stats(pages_body, 'body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbfcd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting stats calculation for title...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished title: avgdl=3.8594021650960557, est_count=6348910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'title_stats.pkl'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_field_stats(pages_title, 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141c3231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting stats calculation for anchor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:==============>                                           (2 + 6) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished anchor: avgdl=95.7779291248419, est_count=6348910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'anchor_stats.pkl'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_field_stats(pages_links, 'anchor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ebf8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload avg len to storage\n",
    "files = [\"anchor_stats.pkl\",\"title_stats.pkl\", \"body_stats.pkl\"]\n",
    "for file_name in files:\n",
    "    index_dst = f'gs://{data_bucket_name}/{storage_output_folder}/stats/{file_name}.pkl'\n",
    "    !gsutil cp $file_name $index_dst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b506ab25",
   "metadata": {},
   "source": [
    "### DOC_ID -> TITLE Mapping - uploading a parquet file for pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2641b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# download mapping between doc_id to title for the search_frontend api to replace each doc id in result to a real title - will be read with pandas later\n",
    "spark.read.parquet(prod_file_name).select(\"title\", \"id\").write.mode(\"overwrite\").parquet(f\"gs://{data_bucket_name}/{storage_output_folder}/titles_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2108158a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prod/titles_df/\n",
      "prod/titles_df/_SUCCESS\n",
      "Downloading titles_df/_SUCCESS...\n",
      "prod/titles_df/part-00000-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00000-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00002-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00002-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00005-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00005-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00007-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00007-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00009-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00009-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00010-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00010-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00012-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00012-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00013-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00013-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00015-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00015-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00016-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00016-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00017-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00017-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00018-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00018-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00019-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00019-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00021-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00021-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00022-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00022-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00024-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00024-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00025-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00025-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00026-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00026-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00027-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00027-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00028-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00028-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00030-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00030-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00031-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00031-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00032-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00032-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00033-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00033-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00034-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00034-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00035-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00035-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00036-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00036-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00037-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00037-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00038-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00038-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00039-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00039-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00040-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00040-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00041-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00041-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00042-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00042-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00043-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00043-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00044-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00044-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00045-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00045-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00046-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00046-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00047-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00047-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00048-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00048-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00049-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00049-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00050-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00050-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00051-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00051-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00052-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00052-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00053-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00053-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00054-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00054-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00055-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00055-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00057-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00057-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00060-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00060-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00063-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00063-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00067-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00067-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00071-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00071-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00075-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00075-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00079-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00079-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00083-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00083-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00110-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00110-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00111-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00111-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00115-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00115-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00118-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00118-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00120-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00120-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00121-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00121-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n",
      "prod/titles_df/part-00122-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet\n",
      "Downloading titles_df/part-00122-4050438b-a9de-4ca6-b56c-536e8d275a91-c000.snappy.parquet...\n"
     ]
    }
   ],
   "source": [
    "# download df code for prediction in search frontend api\n",
    "def download_df():\n",
    "    prefix = f\"{storage_output_folder}/titles_df\"\n",
    "    blobs = storage.Client().bucket(data_bucket_name).list_blobs(prefix=prefix)\n",
    "    for blob in blobs:\n",
    "        print(blob.name)\n",
    "        if blob.name.endswith(\"/\"): \n",
    "            continue # Skip directory placeholders\n",
    "        rel_path = os.path.relpath(blob.name, start=os.path.dirname(prefix))\n",
    "        local_file_path = os.path.join(\".\", rel_path)\n",
    "        os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "        print(f\"Downloading {rel_path}...\")\n",
    "        blob.download_to_filename(local_file_path)\n",
    "download_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23615437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Foster Air Force Base'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index the dataframe and testing the mapping\n",
    "df = pd.read_parquet(\"titles_df\", engine='pyarrow')\n",
    "df_indexed = df.set_index(\"id\")[\"title\"]\n",
    "df_indexed.get(4045403, \"Unknown Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa275688",
   "metadata": {},
   "source": [
    "### Data for BM25 - ID -> Doc Len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a36035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens_precise(text_element, field_type):\n",
    "    \"\"\"\n",
    "    Counts tokens based on the specific tokenizer used for the field.\n",
    "    field_type: 'body' (simple), 'anchor' (stemmed/simple), 'title' (smart/bigrams)\n",
    "    \"\"\"\n",
    "    if text_element is None:\n",
    "        return 0\n",
    "    \n",
    "    # pre proccesing\n",
    "    full_text = \"\"\n",
    "    if isinstance(text_element, (list, tuple)):\n",
    "        # Handle list of Rows or Tuples (common in Anchor text)\n",
    "        text_parts = []\n",
    "        for item in text_element:\n",
    "            if isinstance(item, str):\n",
    "                text_parts.append(item)\n",
    "            elif hasattr(item, \"text\") and item.text: # Spark Row\n",
    "                text_parts.append(item.text)\n",
    "            elif isinstance(item, (list, tuple)) and len(item) > 1: # Tuple\n",
    "                text_parts.append(str(item[1]))\n",
    "        full_text = \" \".join(text_parts)\n",
    "    elif isinstance(text_element, str):\n",
    "        full_text = text_element\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    # Extract base tokens using Regex\n",
    "    tokens = [tok.group().lower() for tok in RE_WORD.finditer(full_text)]\n",
    "    # Filter Stopwords & Length\n",
    "    tokens = [t for t in tokens if t not in all_stopwords and len(t) >= MIN_TOKEN_LEN]\n",
    "    \n",
    "    token_count = len(tokens)\n",
    "\n",
    "    # Only 'title' used tokenize_smart which adds bigrams. \n",
    "    # 'body' and 'anchor' used simple/stem tokenizers without bigrams.\n",
    "    if field_type == 'title':\n",
    "        bigrams_count = 0\n",
    "        for i in range(token_count - 1):\n",
    "            if bigrams_count >= MAX_BIGRAMS:\n",
    "                break\n",
    "            if tokens[i].isalpha() and tokens[i+1].isalpha():\n",
    "                bigrams_count += 1\n",
    "        return token_count + bigrams_count\n",
    "    \n",
    "    # For Body and Anchor, return simple token count\n",
    "    return token_count\n",
    "\n",
    "\n",
    "def calculate_dl_and_save(rdd, field_name):\n",
    "    \"\"\"\n",
    "    Calculates DL, writes directly to Parquet, and prints stats.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Processing {field_name} ---\")\n",
    "    \n",
    "    # Define Mapper\n",
    "    def mapper(row):\n",
    "        # Extract ID and Text robustly\n",
    "        doc_id = row['id'] if hasattr(row, 'id') else row[1]\n",
    "        \n",
    "        # Select content based on known schema columns\n",
    "        if field_name == 'anchor' and hasattr(row, 'anchor_text'):\n",
    "            content = row.anchor_text\n",
    "        elif field_name == 'title' and hasattr(row, 'title'):\n",
    "            content = row.title\n",
    "        elif field_name == 'body' and hasattr(row, 'text'):\n",
    "            content = row.text\n",
    "        else:\n",
    "            content = row[0] # Fallback for tuple (text, id)\n",
    "\n",
    "        # Count tokens using the logic specific to the field type\n",
    "        length = count_tokens_precise(content, field_type=field_name)\n",
    "        return (int(doc_id), int(length))\n",
    "\n",
    "    # Create DataFrame (id, len)\n",
    "    dl_rdd = rdd.map(mapper)\n",
    "    schema = StructType([\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"len\", IntegerType(), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame(dl_rdd, schema)\n",
    "\n",
    "    # Define Output Path\n",
    "    output_path = f\"gs://{data_bucket_name}/{storage_output_folder}/stats/dl_{field_name}.parquet\" # remove .parquet if not work\n",
    "    \n",
    "    # Write to Parquet (Action that triggers the job)\n",
    "    print(f\"Writing to: {output_path}\")\n",
    "    df.write.mode(\"overwrite\").parquet(output_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85d9ec85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing title ---\n",
      "Writing to: gs://ir-final-project-bucket/prod/stats/dl_title.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "calculate_dl_and_save(pages_title, 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8233fb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing anchor ---\n",
      "Writing to: gs://ir-final-project-bucket/prod/stats/dl_anchor.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "calculate_dl_and_save(pages_links, 'anchor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcbacf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing body ---\n",
      "Writing to: gs://ir-final-project-bucket/prod/stats/dl_body.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "calculate_dl_and_save(pages_body, 'body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6c3190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/_SUCCESS...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00000-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00002-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00005-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00013-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00027-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00009-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00028-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00021-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00016-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00010-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00015-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00007-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00022-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00012-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00026-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00018-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00025-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00017-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00024-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00019-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00030-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00031-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00032-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00033-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00034-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00035-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00036-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00037-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00038-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00041-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00040-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00039-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00042-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00043-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00045-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00044-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00046-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00047-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00049-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00048-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00051-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00050-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00052-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00053-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00054-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00055-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00057-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00060-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00063-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00067-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00071-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00075-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00083-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00079-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00110-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00111-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00115-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00120-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00118-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00121-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "Copying gs://ir-final-project-bucket/prod/stats/dl_body.parquet/part-00122-cf64c327-2a9c-47ac-ba94-ae9e16ad4cc4-c000.snappy.parquet...\n",
      "| [62/63 files][ 34.0 MiB/ 34.0 MiB]  99% Done                                  \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1517"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate all the data is good\n",
    "src = f\"gs://{data_bucket_name}/{storage_output_folder}/stats/dl_body.parquet\"\n",
    "!gsutil -m cp -r $src .\n",
    "\n",
    "df = pd.read_parquet(\"dl_body.parquet\", engine='pyarrow')\n",
    "df_indexed = df.set_index(\"id\")[\"len\"]\n",
    "df_indexed.get(4045403, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e943252b",
   "metadata": {
    "id": "e943252b"
   },
   "source": [
    "### Search - Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7913d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_inverted_index_smart = read_inverted_index(\"body_inverted_index\")\n",
    "title_inverted_index_smart = read_inverted_index(\"title_inverted_index_smart\")\n",
    "anchor_inverted_index_smart = read_inverted_index(\"ancher_inverted_index_smart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0aef34",
   "metadata": {
    "id": "db0aef34"
   },
   "outputs": [],
   "source": [
    "def get_bm25_scores(query: str):\n",
    "    \"\"\"\n",
    "    Calculates BM25 scores for a given query across multiple index fields \n",
    "    (Body, Title, Anchor text).\n",
    "    \"\"\"\n",
    "    # BM25 Hyperparameters:\n",
    "    # k1: Controls term frequency saturation \n",
    "    # b: Controls document length normalization \n",
    "    k1, b = 1.5, 0.75\n",
    "    # Dictionary to store cumulative scores for each document ID\n",
    "    candidate_scores = defaultdict(float)\n",
    "\n",
    "    # Configuration for Multi-Field Retrieval\n",
    "    fields = [\n",
    "        (body_inverted_index_smart, BODY_STATS, 0.5, simple_tokenize),\n",
    "        (title_inverted_index_smart, TITLE_STATS, 0.3, tokenize_smart),\n",
    "        (anchor_inverted_index_smart, ANCHOR_STATS, 0.2, tokenize_anchor)\n",
    "    ]\n",
    "\n",
    "    # Iterate over each field (Body, Title, Anchor) to calculate partial scores\n",
    "    for index, stats, weight, tokenize_func in fields:\n",
    "        # Tokenize the query using the field-specific tokenizer\n",
    "        query_tokens = tokenize_text(query, tokenize_func)\n",
    "        unique_tokens = np.unique(query_tokens)\n",
    "        # Retrieve field statistics\n",
    "        avgdl = stats['avgdl'] # Average Document Length for this field\n",
    "        dl_dict = stats['DL'] # Dictionary of Document Lengths {doc_id: length}\n",
    "\n",
    "        for term in unique_tokens:\n",
    "            # Check if term exists in the index (Document Frequency)\n",
    "            n_ti = index.df.get(term, 0)\n",
    "            if n_ti == 0:\n",
    "                continue\n",
    "\n",
    "            # Calculate Inverse Document Frequency (IDF)\n",
    "            idf = math.log(1 + (len(TITLES_DICT) - n_ti + 0.5) / (n_ti + 0.5))\n",
    "\n",
    "            try:\n",
    "                # Retrieve the Posting List for the term (list of (doc_id, freq))\n",
    "                pls = index.read_a_posting_list('.', term, data_bucket_name)\n",
    "                for doc_id, freq in pls:\n",
    "                    # Get length of the specific document (default to avgdl if missing)\n",
    "                    doc_len = dl_dict.get(str(doc_id), avgdl)\n",
    "                    # Calculate the BM25 denominator (Length Normalization component)\n",
    "                    denominator = freq + k1 * (1 - b + b * (doc_len / avgdl))\n",
    "                    # Calculate final BM25 score for this term in this document\n",
    "                    score = idf * (freq * (k1 + 1)) / denominator\n",
    "                    # Add weighted score to the candidate's total\n",
    "                    candidate_scores[doc_id] += (score * weight)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return candidate_scores\n",
    "\n",
    "\n",
    "def get_semantic_similarity(query, titles):\n",
    "    \"\"\"\n",
    "    Computes semantic similarity between the query and a list of candidate titles\n",
    "    using a pre-trained Transformer model (e.g., Sentence-BERT).\n",
    "    \"\"\"\n",
    "    if not titles:\n",
    "        return []\n",
    "    \n",
    "    # Encode the query into a dense vector embedding\n",
    "    query_emb = semantic_model.encode(query, convert_to_tensor=True)\n",
    "    # Encode all candidate titles into dense vector embeddings\n",
    "    title_embs = semantic_model.encode(titles, convert_to_tensor=True)\n",
    "    # Calculate Cosine Similarity between the query vector and title vectors\n",
    "    semantic_sims = util.cos_sim(query_emb, title_embs)[0].tolist()\n",
    "    # Returns a list of similarity scores (0.0 to 1.0)\n",
    "    return semantic_sims\n",
    "\n",
    "\n",
    "def rank_candidates(query, top_candidates_scores):\n",
    "    \"\"\"\n",
    "    Performs the final re-ranking of candidates using a Hybrid Score.\n",
    "    Combines Text Match (BM25), Semantic Meaning, Graph Popularity (PageRank), \n",
    "    and User Traffic (PageViews).\n",
    "    \"\"\"\n",
    "    # Extract document IDs from the initial BM25 results\n",
    "    candidate_ids = [int(cid) for cid, _ in top_candidates_scores]\n",
    "\n",
    "    # Retrieve the actual Title strings for semantic comparison\n",
    "    titles = [TITLES_DICT.get(str(cid), \"Unknown\") for cid in candidate_ids]\n",
    "    # Get semantic similarity scores for these titles\n",
    "    semantic_sims = get_semantic_similarity(query, titles)\n",
    "\n",
    "    final_ranked_list = []\n",
    "    # Iterate through candidates to calculate the composite score\n",
    "    for i, cid in enumerate(candidate_ids):\n",
    "        bm25_val = dict(top_candidates_scores).get(cid, 0)\n",
    "        pr_val = PAGERANK_DICT.get(cid, 0)\n",
    "        pv_val = math.log1p(WID2PV.get(cid, 0))\n",
    "        sem_val = semantic_sims[i] if i < len(semantic_sims) else 0\n",
    "\n",
    "        # Calculate Final Weighted Score (Linear Combination)\n",
    "        final_score = (bm25_val * 0.5) + (sem_val * 0.3) + (pr_val * 0.1) + (pv_val * 0.1)\n",
    "        final_ranked_list.append((str(cid), titles[i], final_score))\n",
    "\n",
    "    # Return the list sorted by final score in descending order\n",
    "    return sorted(final_ranked_list, key=lambda x: x[2], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5804a0",
   "metadata": {
    "id": "df5804a0"
   },
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    res = []\n",
    "    query_tokens = tokenize_smart(query)\n",
    "    if query_tokens:\n",
    "        all_bm25_scores = get_bm25_scores(query_tokens)\n",
    "        if all_bm25_scores:\n",
    "            top_150 = sorted(all_bm25_scores.items(), key=lambda x: x[1], reverse=True)[:150]\n",
    "            final_results = rank_candidates(query, top_150)\n",
    "            res = [(res[0], res[1]) for res in final_results[:100]]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac0f8ac",
   "metadata": {
    "id": "9ac0f8ac",
    "outputId": "05ed97fc-fbd2-4ffb-b658-40edd2b4cc36"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4101403', 'Hungarian Air Force'),\n",
       " ('4045403', 'Foster Air Force Base'),\n",
       " ('4085989', 'Congolese Air Force'),\n",
       " ('4070196', 'List of United States Air Force Academy alumni'),\n",
       " ('4087129', 'Air Force Armament Museum'),\n",
       " ('4060580', 'Korat Royal Thai Air Force Base'),\n",
       " ('4106232', 'Troy Calhoun'),\n",
       " ('4094455', 'Terry W. Virts'),\n",
       " ('4094362', 'Gregory H. Johnson'),\n",
       " ('4073376', 'Trudy H. Clark'),\n",
       " ('4094114', 'Eric Boe'),\n",
       " ('4116550', 'RAF Bovingdon'),\n",
       " ('4107630', '414th Expeditionary Reconnaissance Squadron'),\n",
       " ('4118511', 'Waco Custom Cabin series'),\n",
       " ('4072511', 'Weapon systems officer'),\n",
       " ('4048119', 'HQ-7'),\n",
       " ('4060494', 'RAF East Kirkby'),\n",
       " ('4084303', 'James Dutton (astronaut)'),\n",
       " ('4054880', 'Hector McGregor'),\n",
       " ('4076406', 'Jack Ridley (pilot)'),\n",
       " ('4062411', 'RAF Grimsby'),\n",
       " ('4120172', 'CAFB'),\n",
       " ('4061034', 'Stig Wennerström (colonel)'),\n",
       " ('4062612', 'Hugh Saunders'),\n",
       " ('4103767', '423d Air Base Group'),\n",
       " ('4060466', 'RAF Fiskerton'),\n",
       " ('4047145', 'Allahabad Airport'),\n",
       " ('4105603', 'Joe Lombardi'),\n",
       " ('4086060', 'Frank S. Scott'),\n",
       " ('4092808', 'Joint Chiefs of Staff Committee'),\n",
       " ('4046446', 'Cox Field'),\n",
       " ('4102784', 'Narsarsuaq Airport'),\n",
       " ('4095501', 'Chadian Armed Forces'),\n",
       " ('4098797', 'Fulton surface-to-air recovery system'),\n",
       " ('4111069', 'Chief of Defence (Norway)'),\n",
       " ('4112620', 'No. 9 Squadron RAAF'),\n",
       " ('4112670', 'Conspicuous Service Cross (New York)'),\n",
       " ('4059895', 'No. 19 Squadron RAF'),\n",
       " ('4058866', 'Dallas Executive Airport'),\n",
       " ('4080138', '95th Reconnaissance Squadron'),\n",
       " ('4097604', 'Alex Coomber'),\n",
       " ('4050096', 'Andrew W. Marlowe'),\n",
       " ('4084116', 'No. 316 Polish Fighter Squadron'),\n",
       " ('4109463', 'Greenville Technical College'),\n",
       " ('4116481', 'Alexander Löhr'),\n",
       " ('4052870', 'Brownwood Regional Airport'),\n",
       " ('4081156', 'Lockheed Martin P-791'),\n",
       " ('4106963', 'Lee Crooks'),\n",
       " ('4063431', 'Cheddi Jagan International Airport'),\n",
       " ('4060379', 'Geoffrey Barraclough'),\n",
       " ('4067052', 'Operation Wallpaper'),\n",
       " ('4061602', 'List of flags of Norway'),\n",
       " ('4067092', 'Operation Alpha Centauri'),\n",
       " ('4045691', 'Ben Agajanian'),\n",
       " ('4097961', 'Pongumoodu'),\n",
       " ('4046861', 'VMFA-142'),\n",
       " ('4060860', 'John B. Nichols'),\n",
       " ('4048985', 'Mark Frankel'),\n",
       " ('4106216', 'Vought FU'),\n",
       " ('4048091', 'ARCAspace'),\n",
       " ('4047858', 'Hiroshima–Nishi Airport'),\n",
       " ('4069615', 'Ernest Millington'),\n",
       " ('4107368', 'Blaser R93 Tactical'),\n",
       " ('4092617', 'Cristián de la Fuente'),\n",
       " ('4064900', 'North Texas Regional Airport'),\n",
       " ('4084195', 'Al Hollingworth'),\n",
       " ('4072939', 'Irene Kampen'),\n",
       " ('4110380', 'Texas State Highway Loop 1604'),\n",
       " ('4082555', 'Canada Dry One'),\n",
       " ('4055176', 'RAF Logistics Command'),\n",
       " ('4091776', 'Supermarine S.6B'),\n",
       " ('4069251', 'Meanings of minor planet names: 107001–108000'),\n",
       " ('4113179', 'Sino-Soviet Non-Aggression Pact'),\n",
       " ('4061697', 'Harold Burrough'),\n",
       " ('4065521', 'Judy Buenoano'),\n",
       " ('4050800', 'Vélizy-Villacoublay'),\n",
       " ('4093314', 'Fuji T-3'),\n",
       " ('4115268', 'Kawasaki C-1'),\n",
       " ('4105959', 'Osmancık'),\n",
       " ('4102830', 'Nestor Bolum'),\n",
       " ('4068434', 'Olga Horak'),\n",
       " ('4061593', 'Roger Tomlinson'),\n",
       " ('4081756', 'Fort Crook'),\n",
       " ('4117182', 'List of Power Rangers Wild Force episodes'),\n",
       " ('4074700', 'NACA airfoil'),\n",
       " ('4074264', 'B26'),\n",
       " ('4097290', 'Jarret Johnson'),\n",
       " ('4110575', 'Scatha'),\n",
       " ('4060657', 'All American Freeway'),\n",
       " ('4104287', 'Fethiye'),\n",
       " ('4117178', 'List of Power Rangers Time Force episodes'),\n",
       " ('4063055', 'Bruce Maccabee'),\n",
       " ('4107999', 'Anglo-Polish military alliance'),\n",
       " ('4047357', 'Little Easton'),\n",
       " ('4080726', 'Hermann Görtz'),\n",
       " ('4113784', 'Multan International Airport'),\n",
       " ('4054894', 'Paramilitary forces of Pakistan'),\n",
       " ('4107235', 'Chattanooga Times Free Press'),\n",
       " ('4092726', 'Wycliffe College, Gloucestershire'),\n",
       " ('4103794', 'Operation Prone')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check func\n",
    "search(\"im here air force\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
